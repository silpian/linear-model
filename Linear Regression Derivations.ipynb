{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import linear_model\n",
    "from residual_utils import PseudoResidual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume\n",
    "    $$y = X \\beta + \\epsilon$$\n",
    "where $y$ is a $N \\times 1$ vector, $X$ is a $N \\times p$ matrix, and $\\epsilon$ is a $N \\times 1$ vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.59288027],\n",
       "       [0.0100637 ],\n",
       "       [0.4758262 ],\n",
       "       [0.70877039],\n",
       "       [0.04397543],\n",
       "       [0.87952148],\n",
       "       [0.52008142],\n",
       "       [0.03066105],\n",
       "       [0.22441361],\n",
       "       [0.9536757 ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "N = 100\n",
    "p = 10\n",
    "epsilon_stdev = 0.2\n",
    "\n",
    "X = np.random.rand(N, p)\n",
    "true_beta = np.random.rand(p, 1)\n",
    "y = np.matmul(X, true_beta) + epsilon_stdev*np.random.randn(N, 1)\n",
    "true_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimator_accuracy(estimator, error_function='mse'):\n",
    "    print(f\"{error_function} on training data: {estimator.error(y, X, error_function)}\")\n",
    "    print(f\"R squared on training data: Unadjusted: {estimator.r_squared(y, X)} Adjusted: {estimator.adjusted_r_squared(y, X)}\")\n",
    "    print(f\"Sum of absolute differences of estimator to true estimate {np.sum(np.abs(estimator.get_params() - true_beta))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{\\beta} = (X^TX)^{-1} X^T y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on training data: 0.02812353535752292\n",
      "R squared on training data: Unadjusted: 0.9132790554949366 Adjusted: 0.9046069610444303\n",
      "Sum of absolute differences of estimator to true estimate 2.199061742910766\n"
     ]
    }
   ],
   "source": [
    "ols = linear_model.OLS(standardize=True)\n",
    "ols.fit_by_closed_form(y, X)\n",
    "estimator_accuracy(ols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given \n",
    "    $$\\text{ssr} = ||y - X \\beta||_2^2,$$\n",
    "we update $\\beta_n \\to \\beta_{n+1}$ in the opposite direction of the gradient of $\\text{ssr}$ wrt to $\\beta$ at $\\beta_n$.\n",
    "\n",
    "We have\n",
    "    $$d \\text{ssr}(\\beta) = d (y - X \\beta)^T (y - X \\beta) = 2(y - X \\beta)^T (-X d \\beta)$$\n",
    "    $$\\implies D \\text{ssr}(\\beta) = 2(y - X \\beta)^T (-X)$$\n",
    "    $$\\implies \\nabla_\\beta \\text{ssr} = (D \\text{ssr}(\\beta))^T = -2X^T (y - X \\beta).$$\n",
    "Thus, our update rule is\n",
    "    $$\\beta_{n+1} = \\beta_n - \\nabla_{\\beta} \\text{ssr}(\\beta_n) = \\beta_n + 2X^T(y - X \\beta_n).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on training data: 0.028123535358186985\n",
      "R squared on training data: Unadjusted: 0.9132790554928889 Adjusted: 0.9046069610421777\n",
      "Sum of absolute differences of estimator to true estimate 2.1990607628646117\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "ols.fit_by_gradient_descent(y, X, iter=10000)\n",
    "estimator_accuracy(ols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinate Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OLS\n",
    "    $$\\DeclareMathOperator{\\col}{\\text{col}} \\DeclareMathOperator{\\row}{\\text{row}} \\DeclareMathOperator{\\ssr}{\\text{SSR}} \\hat{\\beta} = \\arg \\min \\text{SSR} = \\arg \\min_\\beta ||y - X \\beta||_2^2 = ||y - \\sum_{i=1}^p \\col_i X \\beta_i||_2^2$$\n",
    "    \n",
    "Coordinate descent minimizes the SSR with respect to one coordinate $\\beta_j$ at a time:\n",
    "    $$d \\ssr(\\beta_j) = (- \\col_j X d \\beta_j)^T (y - \\sum_{i=1}^p \\col_i X \\beta_i) + (y - \\sum_{i=1}^p \\col_i X \\beta_i)^T (- \\col_j X d \\beta_j) = -2(y - \\sum_{i=1}^p \\col_i X \\beta_i)^T \\col_j X d \\beta_j$$\n",
    "    $$\\implies \\frac{\\partial \\ssr}{\\partial \\beta_j} = -2(y - \\sum_{i=1}^p \\col_i X \\beta_i)^T \\col_j X.$$\n",
    "Setting the partial derivative to 0,\n",
    "    $$\\frac{\\partial \\ssr}{\\partial \\beta_j} = 0$$\n",
    "    $$\\begin{align*}\n",
    "    \\implies 0 &= (y - \\sum_{i=1}^p \\col_i X \\beta_i)^T \\col_j X \\\\\n",
    "               &= (y - \\sum_{i \\neq j}^p \\col_i X \\beta_i - \\col_j X \\beta_j)^T \\col_j X \\\\\n",
    "               &= (y - \\sum_{i \\neq j}^p \\col_i X \\beta_i)^T \\col_j X - (\\col_j X \\beta_j)^T \\col_j X \\\\\n",
    "               &= (y - \\sum_{i \\neq j}^p \\col_i X \\beta_i)^T \\col_j X - \\beta_j ||col_j X||_2^2 \\\\\n",
    "    \\end{align*}$$\n",
    "    $$\\begin{align*}\n",
    "    \\implies \\beta_j &= \\frac{1}{||\\col_j X||_2^2} (y - \\sum_{i \\neq j}^p \\col_i X \\beta_i)^T \\col_j X \\\\\n",
    "    &= \\frac{1}{||\\col_j X||_2^2} (y - X \\beta + \\col_j X \\beta_j)^T \\col_j X \n",
    "    \\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on training data: 0.02812353535752292\n",
      "R squared on training data: Unadjusted: 0.9132790554949366 Adjusted: 0.9046069610444303\n",
      "Sum of absolute differences of estimator to true estimate 2.1990617429107653\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "ols.fit_by_coordinate_descent(y, X, iter=10000)\n",
    "estimator_accuracy(ols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at iteration: 86\n",
      "mse on training data: 0.029621504067980184\n",
      "R squared on training data: Unadjusted: 0.9091082355331714 Adjusted: 0.9000190590864885\n",
      "Sum of absolute differences of estimator to true estimate 2.3279819814279294\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "gbols = linear_model.GradientBoostedOLS(standardize=True)\n",
    "gbols.fit(y, X, error_function='mse')\n",
    "\n",
    "estimator_accuracy(gbols, error_function='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at iteration: 86\n",
      "mae on training data: 0.13546819839455734\n",
      "R squared on training data: Unadjusted: 0.8958247093561975 Adjusted: 0.8854071802918173\n",
      "Sum of absolute differences of estimator to true estimate 2.482747197793431\n"
     ]
    }
   ],
   "source": [
    "gbols = linear_model.GradientBoostedOLS(standardize=True)\n",
    "gbols.fit(y, X, error_function='mae')\n",
    "\n",
    "estimator_accuracy(gbols, error_function='mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closed Form Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{\\beta}_{\\lambda}^{\\text{ridge}} = (X^T X + \\lambda I_p)^{-1} X^T y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be efficient, we should SVD $X$ to get a faster inverse of the $X^TX$ term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on training data: 0.028130108873833614\n",
      "R squared on training data: Unadjusted: 0.9132587855844885 Adjusted: 0.9045846641429373\n",
      "Sum of absolute differences of estimator to true estimate 2.2084811123921297\n"
     ]
    }
   ],
   "source": [
    "ridge = linear_model.Ridge(standardize=True)\n",
    "ridge.fit_by_closed_form(y, X, reg_param=0.5)\n",
    "estimator_accuracy(ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given \n",
    "    $$\\ssr = ||y - X \\beta||_2^2 + \\lambda ||\\beta||_2^2,$$\n",
    "we update $\\beta_n \\to \\beta_{n+1}$ in the opposite direction of the gradient of $\\ssr$ wrt to $\\beta$ at $\\beta_n$.\n",
    "\n",
    "We have\n",
    "    $$d \\ssr(\\beta) = d (y - X \\beta)^T (y - X \\beta) + d \\lambda \\beta^T \\beta.$$\n",
    "The differential of the regularization term is\n",
    "    $$d \\beta^T \\beta = (d \\beta)^T \\beta + \\beta^T d \\beta = 2 \\lambda \\beta^T d \\beta,$$\n",
    "which, combined with the differential of cost term calculated above, implies\n",
    "    $$\\nabla_\\beta \\ssr = (D \\ssr(\\beta))^T = -2X^T (y - X \\beta) + 2 \\lambda \\beta.$$\n",
    "Thus, our update rule is\n",
    "    $$\\beta_{n+1} = \\beta_n - \\nabla_{\\beta} \\ssr(\\beta_n) = \\beta_n + 2X^T(y - X \\beta_n) - 2 \\lambda \\beta.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n",
      "MSE on training data: 0.028130248440279997\n",
      "R squared on training data: Unadjusted: 0.9132583552213105 Adjusted: 0.9045841907434415\n",
      "Sum of absolute differences of estimator to true estimate 2.1895525522564436\n"
     ]
    }
   ],
   "source": [
    "ridge = linear_model.Ridge(standardize=True)\n",
    "np.random.seed(10)\n",
    "ridge.fit_by_gradient_descent(y, X, reg_param=0.5)\n",
    "estimator_accuracy(ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinate descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lasso\n",
    "    $$\\hat{\\beta}_\\lambda^{\\text{lasso}} = \\arg \\min_\\beta \\ssr$$\n",
    "where\n",
    "    $$\\ssr = \\frac{1}{2N} ||y - X \\beta||_2^2 + \\lambda ||\\beta||_1$$\n",
    "Since we have already calculated the update to $\\hat{\\beta}_j$ for $||y - X \\beta||_2^2$, we will focus on the $L_1$ regularization term here:\n",
    "    $$\\frac{\\partial}{\\partial \\beta_j} \\lambda ||\\beta||_1 = \\frac{\\partial}{\\partial \\beta_j} \\lambda \\sum_{i=1}^p |\\beta_j| = \\begin{cases} \\lambda & \\beta_j > 0 \\\\ -\\lambda & \\beta_j < 0 \\\\ \\text{undef} & \\beta_j = 0 \\end{cases} = g(\\beta_j),$$\n",
    "where we let \n",
    "    $$g(\\beta_j) = \\begin{cases} \\lambda & \\beta_j > 0 \\\\ -\\lambda & \\beta_j < 0 \\\\ \\text{undef} & \\beta_j = 0 \\end{cases}.$$\n",
    "\n",
    "The subdifferential of the L1 regularization term is\n",
    "    $$g(\\beta_j) = \\begin{cases} \\lambda & \\beta_j > 0 \\\\ -\\lambda & \\beta_j < 0 \\\\ [-\\lambda, \\lambda] & \\beta_j = 0 \\end{cases}$$\n",
    "where the change is having the derivative at $\\beta_j = 0$ be a set, instead of being undefined.\n",
    "\n",
    "Using the derivation for the partial derivative of $||y - X \\beta||_2^2$ above, we have\n",
    "    $$\\frac{\\partial}{\\partial \\beta_j} \\ssr = -\\frac{1}{N} (y - \\sum_{i=1}^p \\col_i X \\beta_i)^T \\col_j X + g(\\beta_j)$$\n",
    "    \n",
    "Similar to the above situation for OLS, we set the partial derivative to 0,\n",
    "    $$\\frac{\\partial \\ssr}{\\partial \\beta_j} = 0$$\n",
    "    $$\\begin{align*}\n",
    "    \\implies 0 &= -\\frac{1}{N} (y - \\sum_{i=1}^p \\col_i X \\beta_i)^T \\col_j X + g(\\beta_j) \\\\\n",
    "               &\\vdots \\\\\n",
    "               &= -\\frac{1}{N} (y - \\sum_{i \\neq j}^p \\col_i X \\beta_i)^T \\col_j X + \\beta_j ||col_j X||_2^2 + g(\\beta_j) \\\\\n",
    "    \\end{align*}$$\n",
    "    $$\\begin{align*}\n",
    "    \\implies \\beta_j &= \\frac{1}{||\\col_j X||_2^2} (\\frac{1}{N} (y - \\sum_{i \\neq j}^p \\col_i X \\beta_i)^T \\col_j X - g(\\beta_j)) \\\\\n",
    "    &= \\frac{1}{||\\col_j X||_2^2} ((y - X \\beta + \\col_j X \\beta_j)^T \\col_j X - g(\\beta_j))\n",
    "    \\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subdifferential cases\n",
    "Now we have to go case by case for the subdifferential. Note that we use $\\beta_j'$ to indicate the new/updated $\\beta_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\beta_j > 0$, then\n",
    "    $$\\frac{1}{||\\col_j X||_2^2} (\\frac{1}{N}(y - X \\beta + \\col_j X \\beta_j)^T \\col_j X - g(\\beta_j)) > 0$$\n",
    "    $$\\implies \\frac{1}{N} (y - X \\beta + \\col_j X \\beta_j)^T \\col_j X > g(\\beta_j) = \\lambda$$\n",
    "and\n",
    "    $$\\implies \\beta_j' = \\frac{1}{||\\col_j X||_2^2} (\\frac{1}{N} (y - X \\beta + \\col_j X \\beta_j)^T \\col_j X - \\lambda)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\beta_j < 0$, then\n",
    "    $$\\frac{1}{||\\col_j X||_2^2} (\\frac{1}{N} (y - X \\beta + \\col_j X \\beta_j)^T \\col_j X - g(\\beta_j)) < 0$$\n",
    "    $$\\implies \\frac{1}{N} (y - X \\beta + \\col_j X \\beta_j)^T \\col_j X < g(\\beta_j) = -\\lambda$$\n",
    "and\n",
    "    $$\\implies \\beta_j' = \\frac{1}{||\\col_j X||_2^2} (\\frac{1}{N} (y - X \\beta + \\col_j X \\beta_j)^T \\col_j X + \\lambda)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\beta_j = 0$, then\n",
    "    $$\\frac{1}{||\\col_j X||_2^2} (\\frac{1}{N} (y - X \\beta + \\col_j X \\beta_j)^T \\col_j X - g(\\beta_j)) = 0$$\n",
    "    $$\\implies -\\lambda \\leq \\frac{1}{N} (y - X \\beta + \\col_j X \\beta_j)^T \\col_j X \\leq \\lambda$$\n",
    "and\n",
    "    $$\\implies \\beta_j' = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarized, we have the update rule\n",
    "    $$\\beta_j = \\begin{cases}\n",
    "    \\frac{1}{||\\col_j X||_2^2} (\\frac{1}{N} (y - X \\beta + \\col_j X \\beta_j)^T \\col_j X - \\lambda) & \\frac{1}{N} (y - X \\beta + \\col_j X \\beta_j)^T \\col_j X > \\lambda \\\\ \n",
    "    \\frac{1}{||\\col_j X||_2^2} (\\frac{1}{N} (y - X \\beta + \\col_j X \\beta_j)^T \\col_j X + \\lambda) & \\frac{1}{N} (y - X \\beta + \\col_j X \\beta_j)^T \\col_j X < -\\lambda \\\\ \n",
    "    0 & -\\lambda \\leq \\frac{1}{N} (y - X \\beta + \\col_j X \\beta_j)^T \\col_j X \\leq \\lambda\n",
    "    \\end{cases}$$\n",
    "Wwe can rewrite the above using the soft-thresholding function $S()$:\n",
    "    $$\\beta_j = \\frac{1}{||\\col_j X||_2^2} S(\\frac{1}{N} (y - X \\beta + \\col_j X \\beta_j)^T \\col_j X, \\lambda),$$\n",
    "where the soft-thresholding function is defined to be\n",
    "    $$S(a, b) = \\begin{cases} a - b & a > b \\\\ a + b & a < -b \\\\ 0 & -b \\leq a \\leq b \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on training data: 0.02821372927524612\n",
      "R squared on training data: Unadjusted: 0.9130009360610128 Adjusted: 0.9043010296671141\n",
      "Sum of absolute differences of estimator to true estimate 2.224631736418961\n"
     ]
    }
   ],
   "source": [
    "lasso = linear_model.Lasso(standardize=True)\n",
    "np.random.seed(100)\n",
    "lasso.fit_by_coordinate_descent(y, X, reg_param=0.6)\n",
    "estimator_accuracy(lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-One-Out Cross Validation for Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose a grid of P possible values $\\lambda_1, \\dots, \\lambda_P$ for the penalty parameter\n",
    "2. For $i = 1, \\dots, N$, exclude the $i^{\\text{th}}$ observation $(y_i, x_i)$ from the sample\n",
    "    1. Use the remaining $N - 1$ observations to compute $P$ ridge estimates $\\hat{\\beta}_{\\lambda_p, x_i}$ where the subscript $\\lambda_p, x_i$ indicates the chosen penalty parameter is $\\lambda_p$ where $p = 1, \\dots, P$ and the $i^{\\text{th}}$ observation has been excluded\n",
    "    2. Compute $P$ out-of-sample predictions of the excluded observation $\\hat{y}_{\\lambda_p, x_i} = x_i \\hat{\\beta}_{\\lambda_p, x_i}$ for $p = 1, \\dots, P$.\n",
    "3. Compute the MSE of the predictions \n",
    "    $$\\text{MSE}_{\\lambda_p} = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_{\\lambda_p, x_i})^2$$\n",
    "for $p = 1, \\dots, P.$\n",
    "4. Choose $\\lambda^* = \\arg \\min_{\\lambda_p} \\text{MSE}_{\\lambda_p}$ as your optimal $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a maximum value for lasso $\\lambda$ grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update rule\n",
    "    $$\\beta_j = \\begin{cases}\n",
    "    \\frac{1}{||\\col_j X||_2^2} ((y - X \\beta + \\col_j X \\beta_j)^T \\col_j X - \\lambda) & (y - X \\beta + \\col_j X \\beta_j)^T \\col_j X > \\lambda \\\\ \n",
    "    \\frac{1}{||\\col_j X||_2^2} ((y - X \\beta + \\col_j X \\beta_j)^T \\col_j X + \\lambda) & (y - X \\beta + \\col_j X \\beta_j)^T \\col_j X < -\\lambda \\\\ \n",
    "    0 & -\\lambda \\leq (y - X \\beta + \\col_j X \\beta_j)^T \\col_j X \\leq \\lambda\n",
    "    \\end{cases}$$\n",
    "implies $\\beta_j$ stays at $0$ if\n",
    "    $$-\\lambda \\leq (y - X \\beta + \\col_j X \\beta_j)^T \\col_j X \\leq \\lambda \\implies |(y - X \\beta + \\col_j X \\beta_j)^T \\col_j X| \\leq \\lambda$$\n",
    "\n",
    "Since\n",
    "    $$|(y - X \\beta + \\col_j X \\beta_j)^T \\col_j X| \\leq |y^T \\col_j X|,$$\n",
    "then \n",
    "    $$|y^T \\col_j X| \\leq \\lambda \\implies |(y - X \\beta + \\col_j X \\beta_j)^T \\col_j X| \\leq \\lambda.$$\n",
    "Then a suitable maximum value for the lasso $\\lambda$ grid is\n",
    "    $$\\lambda_{\\text{max}} = \\max_{j} |y^T \\col_j X|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter chosen among 30 evenly spaced values from 0 to 4.413968123442395\n",
      "CPU times: user 6.3 s, sys: 91.7 ms, total: 6.39 s\n",
      "Wall time: 6.38 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.45661739208024776"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lasso.choose_reg_param_for_lasso(y, X, grid_size=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
